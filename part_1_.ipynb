{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19a5e1c0-de03-46ac-aafb-c17cdf29f19a",
      "metadata": {
        "id": "19a5e1c0-de03-46ac-aafb-c17cdf29f19a"
      },
      "source": [
        "### Q1 - a: Write a very basic RNN model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424882ad-a8f5-4133-befb-21fe537f09d0",
      "metadata": {
        "id": "424882ad-a8f5-4133-befb-21fe537f09d0",
        "outputId": "a911bcdf-9ed5-4c80-e88e-d63fa8b87349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in d:\\new folder\\lib\\site-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision in d:\\new folder\\lib\\site-packages (0.21.0+cpu)\n",
            "Requirement already satisfied: torchaudio in d:\\new folder\\lib\\site-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: filelock in d:\\new folder\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\new folder\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: networkx in d:\\new folder\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in d:\\new folder\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in d:\\new folder\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: setuptools in d:\\new folder\\lib\\site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in d:\\new folder\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\new folder\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in d:\\new folder\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\new folder\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\new folder\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f476bc8c-459f-44a6-b066-0640c3074ca8",
      "metadata": {
        "id": "f476bc8c-459f-44a6-b066-0640c3074ca8"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert word indices into embeddings\n",
        "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Pass through the RNN\n",
        "        output, hidden = self.rnn(embedded)  # output: (batch_size, seq_length, hidden_dim)\n",
        "\n",
        "        # Apply fully connected layer to all timesteps\n",
        "        final_output = self.fc(output)  # Shape: (batch_size, seq_length, output_dim)\n",
        "\n",
        "        return final_output  # Predict for all timesteps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f07dab8-684f-4b8e-bd9b-6b6d0b9fdb32",
      "metadata": {
        "id": "0f07dab8-684f-4b8e-bd9b-6b6d0b9fdb32"
      },
      "source": [
        "### Q1 - b: Write a simple  general LSTM model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e7fc0a-9084-478a-8957-cbc7b859c28e",
      "metadata": {
        "id": "95e7fc0a-9084-478a-8957-cbc7b859c28e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, fc_dim, output_dim, num_layers=2, dropout=0.5):\n",
        "        super(ImprovedLSTM, self).__init__()\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM Layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(hidden_dim, fc_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_output, (hidden, cell) = self.lstm(x)  # (batch_size, seq_length, hidden_dim)\n",
        "\n",
        "        # Apply the first FC layer to all time steps\n",
        "        x = self.fc1(lstm_output)  # Shape: (batch_size, seq_length, fc_dim)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Final prediction layer\n",
        "        x = self.fc2(x)  # Shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        return x  # Predict a word at every timestep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc2840d-483b-4c48-be5f-ad1623351659",
      "metadata": {
        "id": "ffc2840d-483b-4c48-be5f-ad1623351659",
        "outputId": "6b31f4f7-b484-4aea-9ca0-997d777fbc46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in d:\\new folder\\lib\\site-packages (1.25.3)\n",
            "Requirement already satisfied: torch in d:\\new folder\\lib\\site-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: nltk in d:\\new folder\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in d:\\new folder\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\new folder\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: networkx in d:\\new folder\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in d:\\new folder\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in d:\\new folder\\lib\\site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: setuptools in d:\\new folder\\lib\\site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in d:\\new folder\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\new folder\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in d:\\new folder\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in d:\\new folder\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\new folder\\lib\\site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in d:\\new folder\\lib\\site-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: colorama in d:\\new folder\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\new folder\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf torch nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "308ef93f-4b73-4cbb-972f-d797364506b7",
      "metadata": {
        "id": "308ef93f-4b73-4cbb-972f-d797364506b7",
        "outputId": "58046df1-fde8-48be-e112-1273b803726b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting text from: 02_20_20_Harry_20_Potter_20and_20_Chamber_20of_20_Secrets_882fda9c0d.pdf\n",
            "Extracting text from: Harry_Potter_and_the_Sorcerer_s_Stone_www_libpdf_blog_ir_aba92c0a66.pdf\n",
            "CHAPTER ONE \n",
            "THE WORST BIRTHDAY \n",
            " \n",
            "Not for the first time, an argument had broken out over breakfast at number four, Privet Drive. Mr. \n",
            "Vernon Dursley had been woken in the early hours of the morning by a loud, hooting noise from his \n",
            "nephew Harry's room. \n",
            "“Third time this week!” he roared across the table. “If you can't control that owl, it'll have to go!” \n",
            "Harry tried, yet again, to explain. \n",
            "“She's bored,” he said. “She's used to flying around outside. If I could just let her out at night—” \n",
            "“Do I look stupid?” snarled Uncle Vernon, a bit of fried egg dangling from his bushy mustache. “I \n",
            "know what'll happen if that owl's let out.” \n",
            "He exchanged dark looks with his wife, Petunia. \n",
            "Harry tried to argue back but his words were drowned by a long, loud belch from the Dursleys' son, \n",
            "Dudley. \n",
            "“I want more bacon.” \n",
            "“There's more in the frying pan, sweetums,” said Aunt Petunia, turning misty eyes on her massive son. \n",
            "“We must build you up while we've got the chance... I don't like the soun\n"
          ]
        }
      ],
      "source": [
        "import fitz  # I want to use my own data set, so I need to convert pdf files in to txt\n",
        "import os\n",
        "\n",
        "def extract_text_from_pdfs(pdf_folder):\n",
        "\n",
        "    all_text = \"\"\n",
        "\n",
        "    # Loop through all PDF files in the folder\n",
        "    for filename in sorted(os.listdir(pdf_folder)):  # Ensure correct order\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(pdf_folder, filename)\n",
        "            print(f\"Extracting text from: {filename}\")\n",
        "\n",
        "            # Open the PDF file\n",
        "            doc = fitz.open(pdf_path)\n",
        "\n",
        "            # Read text from each page\n",
        "            for page in doc:\n",
        "                all_text += page.get_text(\"text\") + \" \"  # Extract text\n",
        "\n",
        "            doc.close()  # Close file\n",
        "\n",
        "    return all_text\n",
        "\n",
        "\n",
        "pdf_folder = r\"C:\\Users\\Rose\\Documents\\mcmaster\\semester 2\\NLP\\harry potter data set\"\n",
        "\n",
        "# Extract text from all PDFs\n",
        "corpus = extract_text_from_pdfs(pdf_folder)\n",
        "\n",
        "# Display a preview of extracted text\n",
        "print(corpus[:1000])  # Print the first 1000 characters to check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d2c080-accb-4695-8304-5da31d92194d",
      "metadata": {
        "id": "03d2c080-accb-4695-8304-5da31d92194d",
        "outputId": "bf91b0f2-acbf-4c3d-86fd-1b7894b228f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Rose\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['chapter', 'one', 'the', 'worst', 'birthday', 'not', 'for', 'the', 'first', 'time', 'an', 'argument', 'had', 'broken', 'out', 'over', 'breakfast', 'at', 'number', 'four', 'privet', 'drive', 'mr', 'vernon', 'dursley', 'had', 'been', 'woken', 'in', 'the', 'early', 'hours', 'of', 'the', 'morning', 'by', 'a', 'loud', 'hooting', 'noise', 'from', 'his', 'nephew', 'harrys', 'room', 'third', 'time', 'this', 'week', 'he']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK tokenizer model\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and tokenizes text using NLTK.\n",
        "    \"\"\"\n",
        "    # 1. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove special characters, punctuation, and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Keep only letters and spaces\n",
        "\n",
        "    # 3. Tokenization (splitting text into words)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply tokenization to the extracted corpus\n",
        "tokens = preprocess_text(corpus)\n",
        "\n",
        "# Display first 50 tokens to check\n",
        "print(tokens[:50])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895421d4-fd81-4ee2-954e-0d60b553d5db",
      "metadata": {
        "id": "895421d4-fd81-4ee2-954e-0d60b553d5db",
        "outputId": "672bdbf6-ccf5-46e8-c494-51fe0b30dad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[143, 43, 2, 1056, 867, 42, 26, 2, 129, 88, 51, 3433, 16, 868, 29, 60, 800, 17, 831, 354, 869, 754, 108, 176, 404, 16, 41, 1761, 12, 2, 1281, 832, 6, 2, 414, 68, 5, 437, 4277, 490, 35, 10, 3434, 90, 121, 801, 88, 40, 650, 7]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Build word frequency dictionary\n",
        "word_counts = Counter(tokens)\n",
        "\n",
        "# Create a mapping from words to indices\n",
        "word_to_index = {word: idx + 2 for idx, (word, _) in enumerate(word_counts.most_common())}\n",
        "word_to_index[\"<pad>\"] = 0  # Padding token\n",
        "word_to_index[\"<unk>\"] = 1  # Unknown words\n",
        "\n",
        "# Reverse mapping (index → word) for later decoding\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "# Convert words to indices\n",
        "numerical_tokens = [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in tokens]\n",
        "\n",
        "# Show first 50 token indices\n",
        "print(numerical_tokens[:50])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d391efbf-b695-4c97-964e-7c527e2af85b",
      "metadata": {
        "id": "d391efbf-b695-4c97-964e-7c527e2af85b",
        "outputId": "1d595ac4-1320-4cf9-cadf-b7019c66fd77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5487, 30])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Define sequence length\n",
        "SEQ_LENGTH = 30\n",
        "\n",
        "# Convert text into fixed length sequences\n",
        "sequences = [numerical_tokens[i:i+SEQ_LENGTH] for i in range(0, len(numerical_tokens) - SEQ_LENGTH, SEQ_LENGTH)]\n",
        "\n",
        "# Convert sequences to tensors\n",
        "tensor_sequences = [torch.tensor(seq) for seq in sequences]\n",
        "\n",
        "# Pad sequences to ensure they have the same length\n",
        "padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=word_to_index[\"<pad>\"])\n",
        "\n",
        "# Show shape of final dataset\n",
        "print(padded_sequences.shape)  # Expected: (num_sequences, seqlen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b60dfa-723e-447e-b25b-1944501df33e",
      "metadata": {
        "id": "f8b60dfa-723e-447e-b25b-1944501df33e",
        "outputId": "b31f1438-bbdf-4e0b-f2b5-fde7eae7a97f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample batch input shape: torch.Size([32, 29])\n",
            "Sample batch target shape: torch.Size([32, 29])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        \"\"\"\n",
        "        Custom PyTorch dataset for text sequences.\n",
        "        :param sequences: List of padded sequences (tokens as tensors)\n",
        "        \"\"\"\n",
        "        self.inputs = sequences[:, :-1]  # All words except last\n",
        "        self.targets = sequences[:, 1:]  # Next word prediction (shifted by 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "# Create Dataset\n",
        "dataset = TextDataset(padded_sequences)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Show an example batch\n",
        "sample_batch = next(iter(dataloader))\n",
        "print(\"Sample batch input shape:\", sample_batch[0].shape)  # Expected: (batch_size, SEQ_LENGTH-1)\n",
        "print(\"Sample batch target shape:\", sample_batch[1].shape)  # Expected: (batch_size, SEQ_LENGTH-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a6cd5c-82dd-4de1-ad15-21aeadc207f6",
      "metadata": {
        "id": "b2a6cd5c-82dd-4de1-ad15-21aeadc207f6",
        "outputId": "413e7d6f-5cd8-4eb8-834e-f15f3e9419eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNModel(\n",
            "  (embedding): Embedding(10318, 128)\n",
            "  (rnn): RNN(128, 256, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=10318, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "VOCAB_SIZE = len(word_to_index)  # Vocabulary size\n",
        "EMBEDDING_DIM = 128   # Embedding vector size\n",
        "HIDDEN_DIM = 256      # Hidden state size\n",
        "OUTPUT_DIM = VOCAB_SIZE  # Output match vocabulary size\n",
        "\n",
        "# Initialize model\n",
        "model = RNNModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "# Loss function & optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for word prediction\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf1baa0-00a5-48a0-a4da-8f2aac77bb72",
      "metadata": {
        "id": "dcf1baa0-00a5-48a0-a4da-8f2aac77bb72",
        "outputId": "51c487e0-7612-4fdd-d36a-8838714089ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Train Loss: 7.0570 | Validation Loss: 6.6168 | Validation Accuracy: 7.35%\n",
            "Epoch 2/10 | Train Loss: 6.2914 | Validation Loss: 6.2565 | Validation Accuracy: 9.30%\n",
            "Epoch 3/10 | Train Loss: 5.8795 | Validation Loss: 6.0668 | Validation Accuracy: 10.76%\n",
            "Epoch 4/10 | Train Loss: 5.5620 | Validation Loss: 5.9560 | Validation Accuracy: 11.66%\n",
            "Epoch 5/10 | Train Loss: 5.2912 | Validation Loss: 5.8962 | Validation Accuracy: 12.37%\n",
            "Epoch 6/10 | Train Loss: 5.0450 | Validation Loss: 5.8647 | Validation Accuracy: 13.00%\n",
            "Epoch 7/10 | Train Loss: 4.8157 | Validation Loss: 5.8599 | Validation Accuracy: 13.15%\n",
            "Epoch 8/10 | Train Loss: 4.6013 | Validation Loss: 5.8694 | Validation Accuracy: 13.25%\n",
            "Epoch 9/10 | Train Loss: 4.3988 | Validation Loss: 5.8860 | Validation Accuracy: 13.40%\n",
            "Epoch 10/10 | Train Loss: 4.2105 | Validation Loss: 5.9143 | Validation Accuracy: 13.49%\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into training & validation (90% train, 10% validation)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    ###### TRAINING ######\n",
        "    model.train()\n",
        "    for batch_inputs, batch_targets in train_dataloader:\n",
        "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)  # Shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, VOCAB_SIZE), batch_targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    ###### VALIDATION ######\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        for batch_inputs, batch_targets in val_dataloader:\n",
        "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_inputs)\n",
        "\n",
        "            # Compute validation loss\n",
        "            val_loss = criterion(outputs.view(-1, VOCAB_SIZE), batch_targets.view(-1))\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = torch.argmax(outputs, dim=-1)  # Get the predicted word indices\n",
        "            correct += (predictions == batch_targets).sum().item()  # Count correct predictions\n",
        "            total += batch_targets.numel()  # Total words in validation set\n",
        "\n",
        "    ###### PRINT EPOCH RESULTS ######\n",
        "    train_loss = total_train_loss / len(train_dataloader)\n",
        "    val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct / total * 100  # Convert to percentage\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda1e350-6182-4234-9902-4fd58cac3202",
      "metadata": {
        "id": "eda1e350-6182-4234-9902-4fd58cac3202"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_text(model, seed_text, word_to_index, index_to_word, seq_length=30, num_words=100):\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Tokenize seed text\n",
        "    seed_tokens = [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in seed_text.lower().split()]\n",
        "\n",
        "    # Pad if the seed sentence is shorter than seq_length\n",
        "    while len(seed_tokens) < seq_length:\n",
        "        seed_tokens.insert(0, word_to_index[\"<pad>\"])\n",
        "\n",
        "    generated_words = seed_text.split()  # Start with seed words\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for _ in range(num_words):\n",
        "            # Convert to tensor and reshape for batch dimension\n",
        "            input_tensor = torch.tensor(seed_tokens[-seq_length:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "            # Forward pass to get predictions\n",
        "            output = model(input_tensor)\n",
        "\n",
        "            # Get the most probable next word\n",
        "            next_word_index = torch.argmax(output[:, -1, :]).item()\n",
        "            next_word = index_to_word.get(next_word_index, \"<unk>\")  # Convert index to word\n",
        "\n",
        "            # Append the predicted word\n",
        "            generated_words.append(next_word)\n",
        "\n",
        "            # Update the input sequence with the new word\n",
        "            seed_tokens.append(next_word_index)\n",
        "\n",
        "    return \" \".join(generated_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2fa2549-2c63-4a1d-bf0a-c69efd730705",
      "metadata": {
        "id": "f2fa2549-2c63-4a1d-bf0a-c69efd730705",
        "outputId": "6d3f2ed8-0c8e-4d32-a422-53fd4135ed26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text:\n",
            "harry looked at the snake and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the door of the stairs and the\n"
          ]
        }
      ],
      "source": [
        "# Example seed sentence\n",
        "seed_sentence = \"harry looked at \"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(model, seed_sentence, word_to_index, index_to_word, seq_length=30, num_words=100)\n",
        "\n",
        "# Display generated text\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135605dd-5d5a-4b87-b274-04637012541e",
      "metadata": {
        "id": "135605dd-5d5a-4b87-b274-04637012541e"
      },
      "source": [
        "### Q5:Analyzing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "324b2342-2d54-400b-9a8f-5d9b8c53377d",
      "metadata": {
        "id": "324b2342-2d54-400b-9a8f-5d9b8c53377d"
      },
      "source": [
        "### Q5-1: update the hyper parameter too see the impact on the result of RNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cb5fd28-ac50-4618-bc05-7f58b72c1a05",
      "metadata": {
        "id": "7cb5fd28-ac50-4618-bc05-7f58b72c1a05",
        "outputId": "f3fe09f4-39de-4904-8688-2c65dec72467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNModel(\n",
            "  (embedding): Embedding(10318, 256)\n",
            "  (rnn): RNN(256, 512, batch_first=True)\n",
            "  (fc): Linear(in_features=512, out_features=10318, bias=True)\n",
            ")\n",
            "Epoch 1/10 | Train Loss: 7.0468 | Validation Loss: 6.5171 | Validation Accuracy: 7.91%\n",
            "Epoch 2/10 | Train Loss: 6.2351 | Validation Loss: 6.2064 | Validation Accuracy: 9.39%\n",
            "Epoch 3/10 | Train Loss: 5.8483 | Validation Loss: 6.0270 | Validation Accuracy: 10.78%\n",
            "Epoch 4/10 | Train Loss: 5.5369 | Validation Loss: 5.9194 | Validation Accuracy: 11.35%\n",
            "Epoch 5/10 | Train Loss: 5.2631 | Validation Loss: 5.8514 | Validation Accuracy: 12.26%\n",
            "Epoch 6/10 | Train Loss: 5.0089 | Validation Loss: 5.8143 | Validation Accuracy: 12.59%\n",
            "Epoch 7/10 | Train Loss: 4.7671 | Validation Loss: 5.7945 | Validation Accuracy: 12.88%\n",
            "Epoch 8/10 | Train Loss: 4.5345 | Validation Loss: 5.7946 | Validation Accuracy: 12.92%\n",
            "Epoch 9/10 | Train Loss: 4.3092 | Validation Loss: 5.8046 | Validation Accuracy: 13.22%\n",
            "Epoch 10/10 | Train Loss: 4.0958 | Validation Loss: 5.8277 | Validation Accuracy: 13.42%\n"
          ]
        }
      ],
      "source": [
        "# training with new hyper parameters:\n",
        "# New hyperparameters\n",
        "VOCAB_SIZE = len(word_to_index)\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "OUTPUT_DIM = VOCAB_SIZE\n",
        "\n",
        "# Initialize new RNN Model\n",
        "model = RNNModel(vocab_size=VOCAB_SIZE,\n",
        "                 embedding_dim=EMBEDDING_DIM,\n",
        "                 hidden_dim=HIDDEN_DIM,\n",
        "                 output_dim=OUTPUT_DIM)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer for training\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005 )\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "\n",
        "# Training RNN one more tmie:\n",
        "# Split dataset into training & validation (90% train, 10% validation)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Keeping it simple\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    #TRAINING #\n",
        "    model.train()\n",
        "    for batch_inputs, batch_targets in train_dataloader:\n",
        "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)  # Shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.view(-1, VOCAB_SIZE), batch_targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # VALIDATION #\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        for batch_inputs, batch_targets in val_dataloader:\n",
        "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_inputs)\n",
        "\n",
        "            # Compute validation loss\n",
        "            val_loss = criterion(outputs.view(-1, VOCAB_SIZE), batch_targets.view(-1))\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = torch.argmax(outputs, dim=-1)  # Get the predicted word indices\n",
        "            correct += (predictions == batch_targets).sum().item()  # Count correct predictions\n",
        "            total += batch_targets.numel()  # Total words in validation set\n",
        "\n",
        "    # PRINT EPOCH RESULTS\n",
        "    train_loss = total_train_loss / len(train_dataloader)\n",
        "    val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct / total * 100  # Convert to percentage\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6147243-63f8-40bc-8ac6-6eb4854734d8",
      "metadata": {
        "id": "d6147243-63f8-40bc-8ac6-6eb4854734d8",
        "outputId": "e4402302-b78a-4dd4-d7db-af8ab1e0922a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text:\n",
            "harry looked at the back of the library and then he heard the door and the snake had to be a lot of the library he said harry and ron and hermione were planning to the floor to the chamber of secrets for saken harry potter said ron in the world of the forest and the monster of the forest and the monster of the forest and the snake had been a long black traveling his head and the snake was the only one who had been a long thin package in the air and the other hand was a large black cloak\n"
          ]
        }
      ],
      "source": [
        "# Example seed sentence\n",
        "seed_sentence = \"harry looked at\"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(model, seed_sentence, word_to_index, index_to_word, seq_length=30, num_words=100)\n",
        "\n",
        "# Display generated text\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e02aac-df23-4352-8c46-93f69f75da01",
      "metadata": {
        "id": "c0e02aac-df23-4352-8c46-93f69f75da01"
      },
      "outputs": [],
      "source": [
        "\"\"\"Analyzinng : As we can see the first text generated by RNN does not very make sense.\n",
        "it looks like the real story when you read the first few sentences but when keep reading it\n",
        "looks like a text that random words were sitted next to each other without any special meaning.In other words,\n",
        "it is not coherent and is full of repetative words like \"door\" and \"snake\" it shows model is\n",
        "not capable of generating a passage with more variety. After updating the hyperparameters, I was expecting improvement,\n",
        "at least it is not keep repeating itself.\n",
        "although we can not see any significant change in loss validation and ... but the results are way better!\n",
        "in the new version it seems to be more dynamic like a human writing \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0f7d64-589a-4707-85c1-8a94cf394af5",
      "metadata": {
        "id": "6a0f7d64-589a-4707-85c1-8a94cf394af5",
        "outputId": "35f39066-81b5-49a5-eab9-7cc3087adf7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ImprovedLSTM(\n",
            "  (embedding): Embedding(10318, 256)\n",
            "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=10318, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialize LSTM model (same hyperparameters as RNN for fair comparison)\n",
        "model = ImprovedLSTM(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    fc_dim=128,  # Same as before\n",
        "    output_dim=VOCAB_SIZE,  # Predict next word from vocabulary\n",
        "    num_layers=2,\n",
        "    dropout=0.5\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function (same as RNN)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer (same as RNN)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b53600-8065-4773-941b-27e8dd5176ad",
      "metadata": {
        "id": "01b53600-8065-4773-941b-27e8dd5176ad",
        "outputId": "c4693bc4-118c-4bbc-a470-745a2b9cfad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 | Train Loss: 8.4431 | Validation Loss: 7.1369 | Validation Accuracy: 4.40%\n",
            "Epoch 2/20 | Train Loss: 7.2676 | Validation Loss: 6.9093 | Validation Accuracy: 4.42%\n",
            "Epoch 3/20 | Train Loss: 7.0748 | Validation Loss: 6.8546 | Validation Accuracy: 4.42%\n",
            "Epoch 4/20 | Train Loss: 6.9925 | Validation Loss: 6.8215 | Validation Accuracy: 4.43%\n",
            "Epoch 5/20 | Train Loss: 6.9338 | Validation Loss: 6.7920 | Validation Accuracy: 4.43%\n",
            "Epoch 6/20 | Train Loss: 6.8905 | Validation Loss: 6.7749 | Validation Accuracy: 4.43%\n",
            "Epoch 7/20 | Train Loss: 6.8615 | Validation Loss: 6.7680 | Validation Accuracy: 4.43%\n",
            "Epoch 8/20 | Train Loss: 6.8408 | Validation Loss: 6.7621 | Validation Accuracy: 4.43%\n",
            "Epoch 9/20 | Train Loss: 6.8197 | Validation Loss: 6.7569 | Validation Accuracy: 4.43%\n",
            "Epoch 10/20 | Train Loss: 6.8029 | Validation Loss: 6.7524 | Validation Accuracy: 4.43%\n",
            "Epoch 11/20 | Train Loss: 6.7905 | Validation Loss: 6.7507 | Validation Accuracy: 4.43%\n",
            "Epoch 12/20 | Train Loss: 6.7766 | Validation Loss: 6.7515 | Validation Accuracy: 4.43%\n",
            "Epoch 13/20 | Train Loss: 6.7669 | Validation Loss: 6.7484 | Validation Accuracy: 4.43%\n",
            "Epoch 14/20 | Train Loss: 6.7582 | Validation Loss: 6.7457 | Validation Accuracy: 4.43%\n",
            "Epoch 15/20 | Train Loss: 6.7493 | Validation Loss: 6.7435 | Validation Accuracy: 4.43%\n",
            "Epoch 16/20 | Train Loss: 6.7372 | Validation Loss: 6.7400 | Validation Accuracy: 4.42%\n",
            "Epoch 17/20 | Train Loss: 6.7273 | Validation Loss: 6.7347 | Validation Accuracy: 4.47%\n",
            "Epoch 18/20 | Train Loss: 6.7155 | Validation Loss: 6.7306 | Validation Accuracy: 4.65%\n",
            "Epoch 19/20 | Train Loss: 6.7014 | Validation Loss: 6.7231 | Validation Accuracy: 4.80%\n",
            "Epoch 20/20 | Train Loss: 6.6856 | Validation Loss: 6.7139 | Validation Accuracy: 4.87%\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 20  # Increase number of epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImprovedLSTM(vocab_size=VOCAB_SIZE, embedding_dim=100, hidden_dim=256, fc_dim=128, output_dim=VOCAB_SIZE)\n",
        "model.to(device)\n",
        "\n",
        "# Use AdamW optimizer and lower learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for input_sequences, target_labels in train_dataloader:\n",
        "        input_sequences, target_labels = input_sequences.to(device), target_labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        model_predictions = model(input_sequences)  # Shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Ensure loss calculation uses correct shapes\n",
        "        loss = criterion(model_predictions.reshape(-1, VOCAB_SIZE), target_labels.reshape(-1))  # Flatten both\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for input_sequences, target_labels in val_dataloader:\n",
        "            input_sequences, target_labels = input_sequences.to(device), target_labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            model_predictions = model(input_sequences)\n",
        "\n",
        "            # Compute validation loss\n",
        "            val_loss = criterion(model_predictions.reshape(-1, VOCAB_SIZE), target_labels.reshape(-1))\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = torch.argmax(model_predictions, dim=-1)  # Shape: (batch_size, seq_length)\n",
        "            correct += (predictions == target_labels).sum().item()\n",
        "            total += target_labels.numel()\n",
        "\n",
        "    # Print epoch results\n",
        "    train_loss = total_train_loss / len(train_dataloader)\n",
        "    val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct / total * 100  # Convert accuracy to percentage\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab96a2e-cf34-4e5e-9bf6-90b04eac2ba1",
      "metadata": {
        "id": "3ab96a2e-cf34-4e5e-9bf6-90b04eac2ba1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_text(model, seed_text, word_to_index, index_to_word, seq_length=30, num_words=100):\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Tokenize seed text\n",
        "    seed_tokens = [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in seed_text.lower().split()]\n",
        "\n",
        "    # Pad if the seed sentence is shorter than seq_length\n",
        "    while len(seed_tokens) < seq_length:\n",
        "        seed_tokens.insert(0, word_to_index[\"<pad>\"])\n",
        "\n",
        "    generated_words = seed_text.split()  # Start with seed words\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for _ in range(num_words):\n",
        "            # Convert to tensor and reshape for batch dimension\n",
        "            input_tensor = torch.tensor(seed_tokens[-seq_length:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "            # Forward pass to get predictions\n",
        "            output = model(input_tensor)\n",
        "\n",
        "            # Get the most probable next word\n",
        "            next_word_index = torch.argmax(output[:, -1, :]).item()\n",
        "            next_word = index_to_word.get(next_word_index, \"<unk>\")  # Convert index to word\n",
        "\n",
        "            # Append the predicted word\n",
        "            generated_words.append(next_word)\n",
        "\n",
        "            # Update the input sequence with the new word\n",
        "            seed_tokens.append(next_word_index)\n",
        "\n",
        "    return \" \".join(generated_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example seed sentence\n",
        "seed_sentence = \"harry looked at chamber and saw jenny\"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(model, seed_sentence, word_to_index, index_to_word, seq_length=30, num_words=100)\n",
        "\n",
        "# Display generated text\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "pXjyJe2COhsd"
      },
      "id": "pXjyJe2COhsd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3eee1fcf-c609-4489-9de7-a4a11e848ee7",
      "metadata": {
        "id": "3eee1fcf-c609-4489-9de7-a4a11e848ee7"
      },
      "source": [
        "### analyzing LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefb9eb8-72e0-4913-a99e-8876f9f5d0bc",
      "metadata": {
        "id": "aefb9eb8-72e0-4913-a99e-8876f9f5d0bc"
      },
      "outputs": [],
      "source": [
        "\"\"\" well the LSTM didnt work out well because I think the data set is very small for this model.\n",
        "it is repeating it self I tried different learning rated but didn't see any significant improvement here. in my opinion for small data sets\n",
        "small models are better. it is totally repeating itself and at the begining the results were better but i decided to make the lstm model mode complicated\n",
        "to find a better results but apparently with my specific data set which is a small one, harry potter 1 & 2 it is not a good one! so myresults show that\n",
        "although LSTM is a better model in comparison with RNN\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cad0b13-e575-4b27-9e5d-82e67740578a",
      "metadata": {
        "id": "5cad0b13-e575-4b27-9e5d-82e67740578a",
        "outputId": "04585da2-9d69-4276-c6f4-61ec65bf2b49"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'rnn_losses' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[53], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(rnn_losses) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), rnn_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN (LR=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_rnn_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(lstm_losses) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), lstm_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM (LR=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_lstm_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'rnn_losses' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualization of Loss Curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(rnn_losses) + 1), rnn_losses, label=f'RNN (LR={best_rnn_lr})', color='blue')\n",
        "plt.plot(range(1, len(lstm_losses) + 1), lstm_losses, label=f'LSTM (LR={best_lstm_lr})', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title('Training Loss Curves for RNN and LSTM')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot comparison across learning rates\n",
        "plt.figure(figsize=(12, 6))\n",
        "for lr, losses in rnn_results.items():\n",
        "    plt.plot(range(1, len(losses) + 1), losses, label=f'RNN LR={lr}', linestyle='--')\n",
        "for lr, losses in lstm_results.items():\n",
        "    plt.plot(range(1, len(losses) + 1), losses, label=f'LSTM LR={lr}', linestyle='-')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title('Loss Curves Across Different Learning Rates')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}